\relax 
\citation{jmlr/LanckrietCBGJ03}
\citation{icml/BachLJ04}
\citation{nips/SonnenburgRS05}
\citation{jmlr/SonnenburgRSS06}
\citation{cvpr/DuanTXM09}
\citation{cvpr/SunWYCCL09}
\citation{iccv/VedaldiGVZ09}
\citation{corr/MaoT11}
\citation{nips/CristianiniSEK01}
\citation{jmlr/LanckrietCBGJ03}
\citation{nips/ZhuKGL04}
\citation{nips/ZhangA05}
\citation{icml/KulisSD06}
\citation{icml/HoiJL07}
\citation{jmlr/KulisSD09}
\citation{aistats/LiFDSW09}
\citation{uai/MaoT10}
\citation{icml/HoiJL07}
\citation{icml/ZhuangTH09}
\citation{civr/ZhuangH10}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Simple Non-Parametric Kernel Learning Algorithms}{39}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chp:npkl}{{3}{39}}
\citation{nips/SongSBG07}
\citation{aistats/ShawJ07}
\citation{icml/ShawJ09}
\citation{icml/HoiJL07}
\citation{icml/HoiJL07}
\citation{icml/ZhuangTH09}
\citation{icml/Kwok03}
\citation{icml/KulisSD06}
\citation{icml/HoiJL07}
\citation{nips/CristianiniSEK01}
\citation{icml/KulisSD06}
\citation{icml/HoiJL07}
\citation{icml/HoiJ08}
\citation{icml/SindhwaniNB05}
\citation{icml/HoiJL07}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Non-Parametric Kernel Learning from Pairwise Constraints}{41}}
\newlabel{sec:NPK}{{3.1}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Side / Label Information}{41}}
\newlabel{eqn:side_info}{{\unhbox \voidb@x \hbox {\rm  3.1}}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Locality Preserving Regularization}{41}}
\newlabel{sec:laplace}{{3.1.2}{41}}
\citation{icml/HoiJL07}
\citation{Boyd}
\newlabel{eqn:reg}{{\unhbox \voidb@x \hbox {\rm  3.2}}{42}}
\newlabel{eqn:laplace}{{\unhbox \voidb@x \hbox {\rm  3.3}}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Formulation of Non-Parametric Kernel Learning}{42}}
\newlabel{eqn:ori-npk-obj}{{\unhbox \voidb@x \hbox {\rm  3.4}}{42}}
\citation{nips/CristianiniSEK01}
\citation{jmlr/LanckrietCBGJ03}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}SimpleNPKL: Simple Non-Parametric Kernel Learning}{43}}
\newlabel{sec:simpleNPKL}{{3.2}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Regularization on ${\bf  K}$}{43}}
\newlabel{eqn:capacity}{{\unhbox \voidb@x \hbox {\rm  3.5}}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}SimpleNPKL with Linear Loss}{43}}
\newlabel{eqn:npk-lin}{{\unhbox \voidb@x \hbox {\rm  3.6}}{43}}
\newlabel{prop:sdp-eigen_decom}{{3.26}{43}}
\citation{mp/AlizadehHO97}
\newlabel{eqn:AB-sdp}{{\unhbox \voidb@x \hbox {\rm  3.7}}{44}}
\newlabel{eqn:optimal_K}{{\unhbox \voidb@x \hbox {\rm  3.8}}{44}}
\newlabel{eq:ctr}{{\unhbox \voidb@x \hbox {\rm  3.9}}{44}}
\newlabel{eq:obj}{{\unhbox \voidb@x \hbox {\rm  3.10}}{44}}
\newlabel{eq:kkt1}{{\unhbox \voidb@x \hbox {\rm  3.11}}{44}}
\newlabel{eq:kkt2}{{\unhbox \voidb@x \hbox {\rm  3.12}}{44}}
\newlabel{prop:sdp-eigen-decom-2}{{3.27}{45}}
\newlabel{eqn:AB-sdp2}{{\unhbox \voidb@x \hbox {\rm  3.13}}{45}}
\newlabel{eqn:optimal_K3}{{\unhbox \voidb@x \hbox {\rm  3.14}}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}SimpleNPKL with Square Hinge Loss}{46}}
\newlabel{sec:npk-sqr-hinge}{{3.2.3}{46}}
\newlabel{eqn:NPK-obj}{{\unhbox \voidb@x \hbox {\rm  3.15}}{46}}
\newlabel{eqn:NPK-epsilon_cnstrnt}{{\unhbox \voidb@x \hbox {\rm  3.16}}{46}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3.1}Dual Formulation: The Saddle-Point Minimax Problem}{46}}
\newlabel{eqn:NPK-lgrngn}{{\unhbox \voidb@x \hbox {\rm  3.17}}{46}}
\citation{nips/BachH08}
\citation{siam/BoydX05}
\citation{siam/BonnansS98}
\citation{nips/YingCG09}
\newlabel{eqn:saddle}{{\unhbox \voidb@x \hbox {\rm  3.18}}{47}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3.2}Iterative Algorithm}{47}}
\newlabel{lemma:derivative}{{3.3}{47}}
\newlabel{eqn:gra}{{\unhbox \voidb@x \hbox {\rm  3.19}}{47}}
\newlabel{eqn:NPK-obj2}{{\unhbox \voidb@x \hbox {\rm  3.20}}{47}}
\citation{tr/Pataki95}
\citation{mp/AlizadehHO97}
\citation{oms/KrishynanM06}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces SimpleNPKL with (square) hinge loss.}}{48}}
\newlabel{alg:iterative-npk}{{1}{48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3.3}Estimating the Rank of $K$}{48}}
\newlabel{sec:rank}{{3.2.3.3}{48}}
\newlabel{prop:low_rank}{{3.28}{48}}
\citation{siam/BoydX05}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3.4}Determining the Convergence Properties}{49}}
\newlabel{prop:lipschitz}{{3.4}{49}}
\citation{nips/YingCG09}
\citation{mp/Nesterov05}
\newlabel{prop:convergence}{{3.29}{50}}
\newlabel{eqn:inq1}{{\unhbox \voidb@x \hbox {\rm  3.21}}{51}}
\newlabel{eqn:1}{{\unhbox \voidb@x \hbox {\rm  3.22}}{51}}
\newlabel{eqn:2}{{\unhbox \voidb@x \hbox {\rm  3.23}}{51}}
\newlabel{eqn:monotone}{{\unhbox \voidb@x \hbox {\rm  3.24}}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}SimpleNPKL with Square Loss}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}SimpleNPKL with Hinge Loss}{52}}
\citation{ecml/JebaraS06}
\citation{icml/BeygelzimerKL06}
\citation{icml/weinberger08}
\citation{Boyd}
\citation{tr/LehoucqSY98}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Implementation Issues}{53}}
\newlabel{sec:implement}{{3.3}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Building a Sparse Graph Laplacian}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Fast Eigendecomposition by Lanczos Algorithm}{53}}
\newlabel{sec:lanczos}{{3.3.2}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Active Constraint Selection }{54}}
\newlabel{sec:cnstrnt_slctn}{{3.3.3}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Low Rank Approximation of ${\bf  K}$}{54}}
\newlabel{sec:lowrank}{{3.3.4}{54}}
\citation{nips/SongSBG07}
\citation{icml/WeinbergerSS04}
\citation{alt/GrettonBSS05}
\citation{nips/SongSBG07}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Applications of SimpleNPKL}{55}}
\newlabel{sec:discuss}{{3.4}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Colored Maximum Variance Unfolding}{55}}
\newlabel{sec:cmvu}{{3.4.1}{55}}
\newlabel{eqn:mvu}{{\unhbox \voidb@x \hbox {\rm  3.25}}{55}}
\newlabel{eqn:cmvu}{{\unhbox \voidb@x \hbox {\rm  3.25}}{55}}
\citation{aistats/ShawJ07}
\citation{aistats/ShawJ07}
\citation{aistats/ShawJ07}
\citation{icml/ShawJ09}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Minimum Volume Embedding}{56}}
\newlabel{sec:mve}{{3.4.2}{56}}
\newlabel{eqn:mve}{{\unhbox \voidb@x \hbox {\rm  3.26}}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Structure Preserving Embedding}{56}}
\newlabel{sec:spe}{{3.4.3}{56}}
\citation{icml/HoiJL07}
\citation{icml/HoiJL07}
\citation{nips/XingNJR02}
\citation{icml/HoiJL07}
\citation{icml/HoiJL07}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Experiments}{58}}
\newlabel{sec:expt}{{3.5}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Experimental Setup}{58}}
\citation{icml/HoiJL07}
\citation{icml/HoiJL07}
\citation{icml/HoiJL07}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Comparisons on Benchmark Data Sets}{59}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces The statistics of the data sets used in our experiments. }}{60}}
\newlabel{table:data-set}{{3.1}{60}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Clustering accuracy of SimpleNPKL, compared with the results of NPKL in (\unhbox \voidb@x \hbox {\rm  3.4}\hbox {}) using a standard SDP solver, and $k$-means. }}{60}}
\newlabel{table:nineset-clstr-acc}{{3.2}{60}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces CPU time of SimpleNPKL, compared with the results of NPKL in (\unhbox \voidb@x \hbox {\rm  3.4}\hbox {}) using a standard SDP solver. (The best results are in bold and the last ``Speedup" column is listed only for the linear loss case.)}}{61}}
\newlabel{table:nineset-clstr-time}{{3.3}{61}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Results of varying capacity parameter $B$ with fixed $p = 2$ and $C = 1$ on {\it  Iris} and {\it  protein} data sets.}}{61}}
\newlabel{table:iris-B}{{3.4}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Scalability Study on Adult Data Set}{61}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Results of varying $p$ in the $p$-norm regularization over ${\bf  K}$ with fixed $B = 1$ and $C = 1$ on {\it  Iris} and {\it  protein} data sets.}}{61}}
\newlabel{table:iris-p}{{3.5}{61}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces The statistics of the {\em  Adult} database. }}{62}}
\newlabel{table:adult-set}{{3.6}{62}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Evaluation results on {\em  Adult} data set. (The best results are in bold.)}}{62}}
\newlabel{table:adult}{{3.7}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Convergence of SimpleNPKL using square hinge loss on {\em  A1a} and {\em  A2a}. The parameters are $C=1$, $B=N$.}}{62}}
\newlabel{fig:convergence}{{3.1}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Comparisons on Constraint Selection}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Comparisons of clustering accuracy and CPU time by active constraint selection and random selection (constraint selection time is included) on A1a with parameters: $B=N, C=1, k=20,r=0.6$. Using all $3.9K$ constraints directly, the accuracy is $60.8\pm 2.9$ and the CPU time is $81.6$ seconds.}}{63}}
\newlabel{fig:active-cnst-slctn}{{3.2}{63}}
\citation{nips/SongSBG07}
\citation{nips/SongSBG07}
\citation{aistats/ShawJ07}
\citation{icml/ShawJ09}
\citation{aistats/ShawJ07}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.5}Evaluations on Data Embedding Applications}{64}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.5.1}Colored Maximum Variance Unfolding}{64}}
\citation{icml/ShawJ09}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Comparisons of CMVU and CMVU+NPKL on {\em  senate} data set. \textbf  {Time cost of CMVU+NPK is $1.50\pm 0.06$ seconds.} }}{65}}
\@writefile{lof}{\contentsline {subfigure}{\string\numberline{3.3.a}{\ignorespaces Time cost of CMVU with the rank.}}{65}}
\@writefile{lof}{\contentsline {subfigure}{\string\numberline{3.3.b}{\ignorespaces Embedding of CMVU.}}{65}}
\@writefile{lof}{\contentsline {subfigure}{\string\numberline{3.3.c}{\ignorespaces Embedding of CMVU+NPKL.}}{65}}
\newlabel{fig:cmvu-senate}{{3.3}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Comparisons of CMVU and CMVU+NPK on {\em  news20} data set. \textbf  {Time cost of CMVU+NPKL is $120.4 \pm 1.7$ seconds.} }}{65}}
\@writefile{lof}{\contentsline {subfigure}{\string\numberline{3.4.a}{\ignorespaces Time cost of CMVU with the rank.}}{65}}
\@writefile{lof}{\contentsline {subfigure}{\string\numberline{3.4.b}{\ignorespaces Embedding of CMVU.}}{65}}
\@writefile{lof}{\contentsline {subfigure}{\string\numberline{3.4.c}{\ignorespaces Embedding of CMVU+NPKL.}}{65}}
\newlabel{fig:cmvu-news20}{{3.4}{65}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.5.2}Minimum Volume Embedding and Structure Preserving Embedding}{65}}
\citation{aistats/ShawJ07}
\citation{icml/ShawJ09}
\citation{icml/ShawJ09}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Comparisons of CMVU and CMVU+NPKL on {\em  usps} data set. \textbf  {Time cost of CMVU+NPKL is $28.95\pm 1.8$ seconds.} }}{66}}
\@writefile{lof}{\contentsline {subfigure}{\string\numberline{3.5.a}{\ignorespaces Time cost of CMVU with the rank.}}{66}}
\@writefile{lof}{\contentsline {subfigure}{\string\numberline{3.5.b}{\ignorespaces Embedding of CMVU.}}{66}}
\@writefile{lof}{\contentsline {subfigure}{\string\numberline{3.5.c}{\ignorespaces Embedding of CMVU+NPKL.}}{66}}
\newlabel{fig:cmvu-usps}{{3.5}{66}}
\@writefile{lot}{\contentsline {table}{\numberline {3.8}{\ignorespaces $k$-NN classification accuracy on the 2D embedded results. (The best results are bolded.)}}{67}}
\newlabel{table:knn-embedding-acc}{{3.8}{67}}
\@writefile{lot}{\contentsline {table}{\numberline {3.9}{\ignorespaces The evaluation of CPU time cost of different algorithms and the speedup of the SimpleNPKL method over the standard SDP solver. (The best results are bolded.)}}{67}}
\newlabel{table:knn-embedding-time}{{3.9}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Sample photos from two Flickr interest groups: {\em  B\&W} and {\em  Catchy Colors}.}}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The 2D embedding result of Flickr users exclusively belonging to the interest group {\em  B\&W} (blue points) and {\em  Catchy Colors} (red points). The CPU time cost of MVE+NPKL and SPE+NPKL are 27.2 minutes and 196.4 minutes, respectively. }}{69}}
\@writefile{lof}{\contentsline {subfigure}{\string\numberline{3.7.a}{\ignorespaces MVE+NPKL}}{69}}
\@writefile{lof}{\contentsline {subfigure}{\string\numberline{3.7.b}{\ignorespaces SPE+NPKL}}{69}}
\newlabel{fig:flickr}{{3.7}{69}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Summary}{70}}
\newlabel{sec:con}{{3.6}{70}}
\@setckpt{chapter-3}{
\setcounter{page}{72}
\setcounter{equation}{26}
\setcounter{enumi}{6}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{9}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{2}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{9}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
\setcounter{ALC@line}{8}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{1}
\setcounter{theorem}{29}
\setcounter{theorem}{29}
\setcounter{lemma}{4}
\setcounter{lemma}{4}
\setcounter{corollary}{0}
\setcounter{corollary}{0}
\setcounter{definition}{0}
\setcounter{definition}{0}
\setcounter{algo}{0}
\setcounter{algo}{0}
\setcounter{example}{0}
\setcounter{alg}{0}
\setcounter{ccc}{0}
}
